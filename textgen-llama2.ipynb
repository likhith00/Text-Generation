{"cells":[{"cell_type":"markdown","metadata":{},"source":["### This notebook is implementation of <a href=\"https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing#scrollTo=frlSLPin4IJ4\">huggingface tutorial</a>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q datasets accelerate peft bitsandbytes trl==0.4.7"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install git+https://github.com/huggingface/transformers@v4.31-release"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -U datasets"]},{"cell_type":"markdown","metadata":{},"source":["### Restart Kernel Before Running the next Cells"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T05:52:24.429592Z","iopub.status.busy":"2024-02-19T05:52:24.428772Z","iopub.status.idle":"2024-02-19T05:52:39.110842Z","shell.execute_reply":"2024-02-19T05:52:39.110026Z","shell.execute_reply.started":"2024-02-19T05:52:24.429555Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-19 05:52:32.526426: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-19 05:52:32.526554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-19 05:52:32.657705: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"]},{"cell_type":"markdown","metadata":{},"source":["### Login to HuggingFace"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T05:53:05.304733Z","iopub.status.busy":"2024-02-19T05:53:05.304048Z","iopub.status.idle":"2024-02-19T05:53:05.330462Z","shell.execute_reply":"2024-02-19T05:53:05.329414Z","shell.execute_reply.started":"2024-02-19T05:53:05.304703Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a08d785c5b14508a6bbd27fb841ac23","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"markdown","metadata":{},"source":["### Define some necessary Arguments"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T05:53:21.011359Z","iopub.status.busy":"2024-02-19T05:53:21.010632Z","iopub.status.idle":"2024-02-19T05:53:21.022601Z","shell.execute_reply":"2024-02-19T05:53:21.021406Z","shell.execute_reply.started":"2024-02-19T05:53:21.011323Z"},"trusted":true},"outputs":[],"source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","# The instruction dataset to use\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-miniguanaco\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"]},{"cell_type":"markdown","metadata":{},"source":["### Loading Dataset, define Training arguments and start training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load dataset (you can process it here)\n","dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\",split = \"train\")\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model\n","trainer.model.save_pretrained(new_model)"]},{"cell_type":"markdown","metadata":{},"source":["### Run text generation pipeline with our next model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","prompt = \"What is a large language model?\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Empty VRAM\n","del model\n","del pipe\n","del trainer\n","import gc\n","gc.collect()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["### Reload model in FP16 and merge it with LoRA weights"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{},"source":["### Push Model To Hub"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.push_to_hub(new_model, use_temp_dir=False)\n","tokenizer.push_to_hub(new_model, use_temp_dir=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Inferencing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-19T06:20:45.588459Z","iopub.status.busy":"2024-02-19T06:20:45.588093Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-19 06:20:51.880394: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-19 06:20:51.880525: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-19 06:20:52.010201: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"def64744029648d9954dc62fe3dc4cfc","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import pipeline\n","\n","prompt = \"What is a large language model?\"\n","pipe = pipeline(task=\"text-generation\", model=\"likhith231/llama-2-7b-miniguanaco\",max_length=200)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30648,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
